{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import ModelParams, TrainConfig\n",
    "from src.deqn import PolicyNetwork, Trainer, simulate_paths\n",
    "from src.io_utils import make_run_dir, save_run_metadata, save_selected_run, pack_config, save_torch, save_csv, save_json, save_npz, ensure_dir\n",
    "from src.metrics import residual_quality\n",
    "\n",
    "# ---------- config ----------\n",
    "ARTIFACTS_ROOT = os.path.join(\"..\", \"artifacts\")\n",
    "params = ModelParams(device=\"cpu\", dtype=torch.float32)\n",
    "cfg = TrainConfig.mid(seed=0)\n",
    "\n",
    "run_dir = make_run_dir(ARTIFACTS_ROOT, \"taylor\", tag=cfg.mode, seed=cfg.seed)\n",
    "save_run_metadata(run_dir, pack_config(params, cfg, extra={\"policy\":\"taylor\"}))\n",
    "print(\"Run dir:\", run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbar = None  # Taylor training does not need rbar_by_regime\n",
    "flex = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.steady_states import solve_flexprice_sss, solve_taylor_sss\n",
    "flex = solve_flexprice_sss(params)\n",
    "taylor_sss = solve_taylor_sss(params, flex)\n",
    "save_json(os.path.join(run_dir, \"sss.json\"), {\"policy\":\"taylor\",\"by_regime\": taylor_sss.by_regime})\n",
    "\n",
    "print(\"=== FLEX SSS (by regime) ===\")\n",
    "for _s in sorted(flex.by_regime.keys()):\n",
    "    print(f\"Regime {_s}:\")\n",
    "    for _k,_v in flex.by_regime[_s].items():\n",
    "        print(f\"{_k:>20}: {_v}\")\n",
    "\n",
    "print(\"=== TAYLOR SSS (by regime) ===\")\n",
    "for _s in sorted(taylor_sss.by_regime.keys()):\n",
    "    print(f\"Regime {_s}:\")\n",
    "    for _k,_v in taylor_sss.by_regime[_s].items():\n",
    "        print(f\"{_k:>20}: {_v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- model ----------\n",
    "d_in, d_out = 5, 8\n",
    "net = PolicyNetwork(d_in, d_out, hidden=cfg.hidden_layers, activation=cfg.activation)\n",
    "\n",
    "trainer = Trainer(\n",
    "    params=params,\n",
    "    cfg=cfg,\n",
    "    policy=\"taylor\",\n",
    "    net=net,\n",
    "    rbar_by_regime=rbar if \"taylor\"==\"mod_taylor\" else None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- train ----------\n",
    "losses = trainer.train(\n",
    "    commitment_sss=None,\n",
    "    n_path=cfg.n_path,\n",
    "    n_paths_per_step=cfg.n_paths_per_step,\n",
    ")\n",
    "\n",
    "# save weights and log\n",
    "save_torch(os.path.join(run_dir, \"weights.pt\"), trainer.net.state_dict())\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"iter\": np.arange(len(losses)), \"loss\": losses})\n",
    "save_csv(os.path.join(run_dir, \"train_log.csv\"), df)\n",
    "\n",
    "# quality on a fresh validation batch sampled from the model's simulated state distribution\n",
    "# Discretion residuals require autograd through Delta-derivative terms.\n",
    "ctx = torch.enable_grad() if trainer.policy == \"discretion\" else torch.inference_mode()\n",
    "with ctx:\n",
    "    x_val = trainer.simulate_initial_state(int(cfg.val_size), commitment_sss=None)\n",
    "    # optional short burn-in for validation states (kept small; training itself is path-based)\n",
    "    val_burn = int(getattr(cfg, \"val_burn_in\", 200))\n",
    "    for _ in range(val_burn):\n",
    "        x_val = trainer._step_state(x_val)\n",
    "    resid = trainer._residuals(x_val).detach().cpu().numpy()\n",
    "q = residual_quality(resid, tol=getattr(cfg, \"report_tol\", 1e-3))\n",
    "save_json(os.path.join(run_dir, \"train_quality.json\"), q)\n",
    "print(\"Train quality:\", q)\n",
    "\n",
    "# optional: mark this run as selected for results notebook\n",
    "save_selected_run(ARTIFACTS_ROOT, trainer.policy, run_dir)\n",
    "\n",
    "# ---------- simulate ergodic paths ----------\n",
    "x0 = trainer.simulate_initial_state(512, commitment_sss=None)\n",
    "sim = simulate_paths(\n",
    "    params=params,\n",
    "    policy=trainer.policy,\n",
    "    net=trainer.net,\n",
    "    T=20000,\n",
    "    burn_in=2000,\n",
    "    x0=x0,\n",
    "    rbar_by_regime=rbar if trainer.policy==\"mod_taylor\" else None,\n",
    "    compute_implied_i=False,\n",
    "    gh_n=3,\n",
    "    thin=10,\n",
    "    show_progress=True,\n",
    ")\n",
    "save_npz(os.path.join(run_dir, \"sim_paths.npz\"), **sim)\n",
    "print(\"Saved sim_paths to:\", os.path.join(run_dir, \"sim_paths.npz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c054e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Sanity checks (fixed point + residuals) ----------\n",
    "from src.sss_from_policy import switching_policy_sss_by_regime_from_policy\n",
    "from src.sanity_checks import fixed_point_check, residuals_check\n",
    "tay_sss_pol = switching_policy_sss_by_regime_from_policy(params, trainer.net, policy='taylor')\n",
    "fp = fixed_point_check(params, trainer.net, policy='taylor', sss_by_regime=tay_sss_pol.by_regime)\n",
    "rc = residuals_check(params, trainer.net, policy='taylor', sss_by_regime=tay_sss_pol.by_regime)\n",
    "print('Fixed point max |x_next-x| by regime:', {k:v.max_abs_state_diff for k,v in fp.items()})\n",
    "print('Residual check max |res| by regime:', {k:v.max_abs_residual for k,v in rc.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb504caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save SSS from trained policy (paper-faithful) ----------\n",
    "save_json(os.path.join(run_dir, 'sss_policy_fixed_point.json'), {\n",
    "    'policy': 'taylor',\n",
    "    'by_regime': tay_sss_pol.by_regime\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save sanity checks ----------\n",
    "save_json(os.path.join(run_dir, 'sanity_checks.json'), {\n",
    "    'policy': 'taylor',\n",
    "    'fixed_point_max_abs_state_diff': {int(k): float(v.max_abs_state_diff) for k,v in fp.items()},\n",
    "    'residual_max_abs': {int(k): float(v.max_abs_residual) for k,v in rc.items()},\n",
    "    'residuals_by_regime': {int(k): {kk: float(vv) for kk,vv in v.residuals.items()} for k,v in rc.items()},\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
