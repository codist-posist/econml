{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00668d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "def _find_project_root():\n",
    "    here = pathlib.Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / \"src\").is_dir():\n",
    "            return p\n",
    "    # Common Google Colab clone location\n",
    "    cand = pathlib.Path(\"/content/econml\")\n",
    "    if (cand / \"src\").is_dir():\n",
    "        return cand\n",
    "    raise RuntimeError(\"Could not find project root containing src/. If on Colab, clone repo to /content/econml.\")\n",
    "\n",
    "PROJECT_ROOT = _find_project_root()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import ModelParams, TrainConfig\n",
    "from src.deqn import PolicyNetwork, Trainer, simulate_paths\n",
    "from src.io_utils import make_run_dir, save_run_metadata, save_selected_run, pack_config, save_torch, save_csv, save_json, save_npz, ensure_dir\n",
    "from src.metrics import residual_quality\n",
    "\n",
    "# ---------- config ----------\n",
    "ARTIFACTS_ROOT = str(PROJECT_ROOT / \"artifacts\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "params = ModelParams(device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "cfg_seed = 0\n",
    "cfg_probe = TrainConfig.mid(seed=cfg_seed)\n",
    "run_dir = make_run_dir(ARTIFACTS_ROOT, \"discretion\", tag=cfg_probe.mode, seed=cfg_probe.seed)\n",
    "cfg = TrainConfig.mid(seed=cfg_seed, run_dir=run_dir, artifacts_root=ARTIFACTS_ROOT)\n",
    "\n",
    "save_run_metadata(run_dir, pack_config(params, cfg, extra={\"policy\":\"discretion\"}))\n",
    "print(\"Run dir:\", run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbar = None  # only used for mod_taylor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb334ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- model ----------\n",
    "d_in, d_out = 5, 11\n",
    "net = PolicyNetwork(d_in, d_out, hidden=cfg.hidden_layers, activation=cfg.activation)\n",
    "\n",
    "trainer = Trainer(\n",
    "    params=params,\n",
    "    cfg=cfg,\n",
    "    policy=\"discretion\",\n",
    "    net=net,\n",
    "    rbar_by_regime=rbar if \"discretion\"==\"mod_taylor\" else None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- train ----------\n",
    "losses = trainer.train(\n",
    "    commitment_sss=None,\n",
    "    n_path=cfg.n_path,\n",
    "    n_paths_per_step=cfg.n_paths_per_step,\n",
    ")\n",
    "\n",
    "# save weights and log\n",
    "save_torch(os.path.join(run_dir, \"weights.pt\"), trainer.net.state_dict())\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"iter\": np.arange(len(losses)), \"loss\": losses})\n",
    "save_csv(os.path.join(run_dir, \"train_log.csv\"), df)\n",
    "\n",
    "# quality on a fresh validation batch sampled from the model's simulated state distribution\n",
    "# Discretion residuals require autograd through Delta-derivative terms.\n",
    "ctx = torch.enable_grad() if trainer.policy == \"discretion\" else torch.inference_mode()\n",
    "with ctx:\n",
    "    x_val = trainer.simulate_initial_state(int(cfg.val_size), commitment_sss=None)\n",
    "    # optional short burn-in for validation states (kept small; training itself is path-based)\n",
    "    val_burn = int(getattr(cfg, \"val_burn_in\", 200))\n",
    "    for _ in range(val_burn):\n",
    "        x_val = trainer._step_state(x_val)\n",
    "    resid = trainer._residuals(x_val).detach().cpu().numpy()\n",
    "q = residual_quality(resid, tol=getattr(cfg, \"report_tol\", 1e-3))\n",
    "save_json(os.path.join(run_dir, \"train_quality.json\"), q)\n",
    "print(\"Train quality:\", q)\n",
    "\n",
    "# optional: mark this run as selected for results notebook\n",
    "save_selected_run(ARTIFACTS_ROOT, trainer.policy, run_dir)\n",
    "\n",
    "# ---------- simulate ergodic paths ----------\n",
    "x0 = trainer.simulate_initial_state(512, commitment_sss=None)\n",
    "sim = simulate_paths(\n",
    "    params=params,\n",
    "    policy=trainer.policy,\n",
    "    net=trainer.net,\n",
    "    T=20000,\n",
    "    burn_in=2000,\n",
    "    x0=x0,\n",
    "    rbar_by_regime=rbar if trainer.policy==\"mod_taylor\" else None,\n",
    "    compute_implied_i=True,\n",
    "    gh_n=3,\n",
    "    thin=10,\n",
    "    show_progress=True,\n",
    ")\n",
    "save_npz(os.path.join(run_dir, \"sim_paths.npz\"), **sim)\n",
    "print(\"Saved sim_paths to:\", os.path.join(run_dir, \"sim_paths.npz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8db3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SSS from trained policy (paper-faithful) ----------\n",
    "from src.steady_states import solve_discretion_sss_from_policy_switching\n",
    "disc_sss = solve_discretion_sss_from_policy_switching(params, trainer.net)\n",
    "save_json(os.path.join(run_dir, 'sss_policy_fixed_point.json'), {'policy':'discretion','by_regime': disc_sss.by_regime})\n",
    "print('=== DISCRETION SSS (policy fixed point (switching-consistent), by regime) ===')\n",
    "for _s in sorted(disc_sss.by_regime.keys()):\n",
    "    print(f'Regime {_s}:')\n",
    "    for _k,_v in disc_sss.by_regime[_s].items():\n",
    "        print(f'{_k:>20}: {_v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7360219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Sanity checks (fixed-regime mapping + switching-consistent residuals) ----------\n",
    "from src.sanity_checks import fixed_point_check, residuals_check_switching_consistent\n",
    "fp = fixed_point_check(params, trainer.net, policy='discretion', sss_by_regime=disc_sss.by_regime)\n",
    "rc = residuals_check_switching_consistent(params, trainer.net, policy='discretion', sss_by_regime=disc_sss.by_regime)\n",
    "print('Fixed-regime one-step check max |x_next-x| by regime (NOT Table-2 SSS):', {k:v.max_abs_state_diff for k,v in fp.items()})\n",
    "print('Switching-consistent residual check max |res| by regime:', {k:v.max_abs_residual for k,v in rc.items()})\n",
    "print('Residual keys:', list(next(iter(rc.values())).residuals.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save sanity checks ----------\n",
    "save_json(os.path.join(run_dir, 'sanity_checks.json'), {\n",
    "    'policy': 'discretion',\n",
    "    'fixed_regime_one_step_max_abs_state_diff': {int(k): float(v.max_abs_state_diff) for k,v in fp.items()},\n",
    "    'residual_max_abs': {int(k): float(v.max_abs_residual) for k,v in rc.items()},\n",
    "    'residuals_by_regime': {int(k): {kk: float(vv) for kk,vv in v.residuals.items()} for k,v in rc.items()},\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
