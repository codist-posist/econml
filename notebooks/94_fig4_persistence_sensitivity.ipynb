{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa9ad07",
   "metadata": {},
   "source": [
    "# 94_fig4_persistence_sensitivity\n",
    "\n",
    "Figure 4: compare temporary Î¾ shock persistence across two commitment runs (e.g., rho_tau=0.90 vs 0.99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e27613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, numpy as np, torch, matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "\n",
    "def _find_project_root():\n",
    "    here = pathlib.Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / \"src\").is_dir():\n",
    "            return p\n",
    "    # Common Google Colab clone location\n",
    "    cand = pathlib.Path(\"/content/econml\")\n",
    "    if (cand / \"src\").is_dir():\n",
    "        return cand\n",
    "    raise RuntimeError(\"Could not find project root containing src/. If on Colab, clone repo to /content/econml.\")\n",
    "\n",
    "PROJECT_ROOT = _find_project_root()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import ModelParams\n",
    "from src.io_utils import load_json, load_npz, load_torch, load_selected_run, find_latest_run_dir\n",
    "from src.deqn import PolicyNetwork\n",
    "\n",
    "ARTIFACTS_ROOT = os.environ.get(\"ARTIFACTS_ROOT\", str(PROJECT_ROOT / \"artifacts\"))\n",
    "ART = os.path.join(ARTIFACTS_ROOT, \"runs\")\n",
    "print(\"ART:\", ART)\n",
    "\n",
    "def get_run(policy: str) -> str:\n",
    "    rd = load_selected_run(ART, policy)\n",
    "    if rd is None:\n",
    "        rd = find_latest_run_dir(ART, policy)\n",
    "    if rd is None:\n",
    "        raise RuntimeError(f\"No runs found for policy={policy} under {ART}\")\n",
    "    return rd\n",
    "\n",
    "def _parse_dtype(s: str):\n",
    "    if s is None:\n",
    "        return torch.float32\n",
    "    if isinstance(s, torch.dtype):\n",
    "        return s\n",
    "    s = str(s)\n",
    "    if \"float64\" in s:\n",
    "        return torch.float64\n",
    "    if \"float32\" in s:\n",
    "        return torch.float32\n",
    "    if \"bfloat16\" in s:\n",
    "        return torch.bfloat16\n",
    "    return torch.float32\n",
    "\n",
    "def load_params_from_run(run_dir: str, *, device=\"cpu\"):\n",
    "    cfg = load_json(os.path.join(run_dir, \"config.json\"))\n",
    "    p = cfg.get(\"params\", {})\n",
    "    dtype = _parse_dtype(p.get(\"dtype\"))\n",
    "    dev = device if device is not None else p.get(\"device\",\"cpu\")\n",
    "    keep = {k:v for k,v in p.items() if k in ModelParams.__dataclass_fields__}\n",
    "    keep[\"device\"] = dev\n",
    "    keep[\"dtype\"] = dtype\n",
    "    return ModelParams(**keep).to_torch()\n",
    "\n",
    "def load_net_from_run(run_dir: str, d_in: int, d_out: int):\n",
    "    cfg = load_json(os.path.join(run_dir, \"config.json\"))\n",
    "    tc = cfg.get(\"train_cfg\", {})\n",
    "    hidden = tuple(tc.get(\"hidden_layers\", (512,512)))\n",
    "    activation = tc.get(\"activation\", \"selu\")\n",
    "    net = PolicyNetwork(d_in, d_out, hidden=hidden, activation=activation)\n",
    "    state = load_torch(os.path.join(run_dir, \"weights.pt\"), map_location=\"cpu\")\n",
    "    # state is usually a plain state_dict\n",
    "    if isinstance(state, dict) and \"state_dict\" in state:\n",
    "        state = state[\"state_dict\"]\n",
    "    net.load_state_dict(state)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "# --- paper reporting helpers ---\n",
    "ann = lambda x: 400.0*x  # annualized percent (quarterly -> annual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89620cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.steady_states import solve_commitment_sss_from_policy_switching\n",
    "from src.experiments import DeterministicPathSpec, simulate_deterministic_path, calibrate_xi_jump_to_match_pi_impact\n",
    "\n",
    "ART_ROOT = str(PROJECT_ROOT / \"artifacts\")\n",
    "ROOT = os.path.join(ART_ROOT,\"runs\",\"commitment\")\n",
    "\n",
    "def _has_weights(run_dir: str) -> bool:\n",
    "    return os.path.exists(os.path.join(run_dir, \"weights.pt\")) or os.path.exists(os.path.join(run_dir, \"weights_best.pt\"))\n",
    "\n",
    "def list_runs(root):\n",
    "    if not os.path.isdir(root):\n",
    "        return []\n",
    "    runs = []\n",
    "    for d in os.listdir(root):\n",
    "        rd = os.path.join(root, d)\n",
    "        if os.path.isdir(rd) and _has_weights(rd):\n",
    "            runs.append(rd)\n",
    "    runs.sort(key=lambda q: os.path.getmtime(q), reverse=True)\n",
    "    return runs\n",
    "\n",
    "def _is_no_regime(params) -> bool:\n",
    "    return float(params.eta_bar) == 0.0 and float(params.p12) == 0.0 and float(params.p21) == 0.0\n",
    "\n",
    "def _pick_valid_pair(candidates):\n",
    "    cache = {}\n",
    "    def getp(run_dir):\n",
    "        if run_dir not in cache:\n",
    "            cache[run_dir] = load_params_from_run(run_dir)\n",
    "        return cache[run_dir]\n",
    "\n",
    "    for i in range(len(candidates)):\n",
    "        ra = candidates[i]\n",
    "        pa = getp(ra)\n",
    "        if not _is_no_regime(pa):\n",
    "            continue\n",
    "        for j in range(i + 1, len(candidates)):\n",
    "            rb = candidates[j]\n",
    "            pb = getp(rb)\n",
    "            if not _is_no_regime(pb):\n",
    "                continue\n",
    "            if abs(float(pa.rho_tau) - float(pb.rho_tau)) > 1e-12:\n",
    "                return ra, rb, pa, pb\n",
    "    return None, None, None, None\n",
    "\n",
    "runs = []\n",
    "selected = load_selected_run(ART_ROOT, \"commitment\")\n",
    "if selected is not None and os.path.isdir(selected) and _has_weights(selected):\n",
    "    runs.append(selected)\n",
    "for rd in list_runs(ROOT):\n",
    "    if rd not in runs:\n",
    "        runs.append(rd)\n",
    "\n",
    "RUN_A = None\n",
    "RUN_B = None\n",
    "\n",
    "if RUN_A is None or RUN_B is None:\n",
    "    RUN_A, RUN_B, paramsA, paramsB = _pick_valid_pair(runs)\n",
    "    if RUN_A is None:\n",
    "        raise RuntimeError(\n",
    "            \"Need at least two commitment runs with no-regime calibration \"\n",
    "            \"(eta_bar=0, p12=0, p21=0) and different rho_tau.\"\n",
    "        )\n",
    "else:\n",
    "    if not (_has_weights(RUN_A) and _has_weights(RUN_B)):\n",
    "        raise RuntimeError(\"RUN_A/RUN_B must point to runs containing weights.pt or weights_best.pt.\")\n",
    "    paramsA = load_params_from_run(RUN_A)\n",
    "    paramsB = load_params_from_run(RUN_B)\n",
    "\n",
    "print(\"RUN_A:\", RUN_A)\n",
    "print(\"RUN_B:\", RUN_B)\n",
    "\n",
    "# Paper Figure 4 strict requirement: run in the no-regime model.\n",
    "def _require_no_regimes(params, label):\n",
    "    if float(params.eta_bar) != 0.0 or float(params.p12) != 0.0 or float(params.p21) != 0.0:\n",
    "        raise RuntimeError(\n",
    "            f\"{label}: Figure 4 in the paper is defined for the no-regime model only (eta_bar=0, p12=0, p21=0). \"\n",
    "            f\"Current values: eta_bar={params.eta_bar}, p12={params.p12}, p21={params.p21}.\"\n",
    "        )\n",
    "\n",
    "_require_no_regimes(paramsA, 'Run A')\n",
    "_require_no_regimes(paramsB, 'Run B')\n",
    "\n",
    "if abs(float(paramsA.rho_tau) - float(paramsB.rho_tau)) < 1e-12:\n",
    "    raise RuntimeError(\n",
    "        \"Figure 4 requires two different temporary-shock persistence values (rho_tau). \"\n",
    "        f\"Both runs have rho_tau={paramsA.rho_tau}.\"\n",
    "    )\n",
    "\n",
    "netA = load_net_from_run(RUN_A, 7, 13)\n",
    "netB = load_net_from_run(RUN_B, 7, 13)\n",
    "\n",
    "sssA = solve_commitment_sss_from_policy_switching(paramsA, netA)\n",
    "sssB = solve_commitment_sss_from_policy_switching(paramsB, netB)\n",
    "\n",
    "x0A = torch.tensor([[float(sssA.by_regime[0][\"Delta_prev\"]), float(sssA.by_regime[0][\"logA\"]), float(sssA.by_regime[0][\"loggtilde\"]), float(sssA.by_regime[0][\"xi\"]), 0.0,\n",
    "                     float(sssA.by_regime[0][\"vartheta_prev\"]), float(sssA.by_regime[0][\"varrho_prev\"])]], dtype=torch.float32)\n",
    "x0B = torch.tensor([[float(sssB.by_regime[0][\"Delta_prev\"]), float(sssB.by_regime[0][\"logA\"]), float(sssB.by_regime[0][\"loggtilde\"]), float(sssB.by_regime[0][\"xi\"]), 0.0,\n",
    "                     float(sssB.by_regime[0][\"vartheta_prev\"]), float(sssB.by_regime[0][\"varrho_prev\"])]], dtype=torch.float32)\n",
    "\n",
    "T=40\n",
    "target_pi0 = 0.01\n",
    "\n",
    "xiA = calibrate_xi_jump_to_match_pi_impact(paramsA, \"commitment\", netA, x0=x0A, target_pi0=target_pi0, horizon_T=1)\n",
    "xiB = calibrate_xi_jump_to_match_pi_impact(paramsB, \"commitment\", netB, x0=x0B, target_pi0=target_pi0, horizon_T=1)\n",
    "\n",
    "x0A2 = x0A.clone(); x0A2[:,3] += xiA\n",
    "x0B2 = x0B.clone(); x0B2[:,3] += xiB\n",
    "\n",
    "spec = DeterministicPathSpec(T=T, epsA=0.0, epsg=0.0, epst=0.0, regime_path=None)\n",
    "pathA = simulate_deterministic_path(paramsA, \"commitment\", netA, x0=x0A2, spec=spec, compute_implied_i=True)\n",
    "pathB = simulate_deterministic_path(paramsB, \"commitment\", netB, x0=x0B2, spec=spec, compute_implied_i=True)\n",
    "\n",
    "t=np.arange(T+1)\n",
    "plt.figure()\n",
    "plt.plot(t, ann(pathA[\"pi\"][:,0]), label=f\"Run A (rho_tau={paramsA.rho_tau})\")\n",
    "plt.plot(t, ann(pathB[\"pi\"][:,0]), label=f\"Run B (rho_tau={paramsB.rho_tau})\", linestyle=\"--\")\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.title(\"Figure 4: pi persistence (matched impact)\")\n",
    "plt.xlabel(\"t\"); plt.ylabel(\"pi\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf381160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for no-regime configuration and distinct rho_tau are enforced in the previous cell before any simulation.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
