{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7896388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "from dataclasses import replace\n",
    "\n",
    "def _find_project_root():\n",
    "    here = pathlib.Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / \"src\").is_dir():\n",
    "            return p\n",
    "    # Common Google Colab clone location\n",
    "    cand = pathlib.Path(\"/content/econml\")\n",
    "    if (cand / \"src\").is_dir():\n",
    "        return cand\n",
    "    raise RuntimeError(\"Could not find project root containing src/. If on Colab, clone repo to /content/econml.\")\n",
    "\n",
    "PROJECT_ROOT = _find_project_root()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import ModelParams, TrainConfig\n",
    "from src.deqn import PolicyNetwork, Trainer, simulate_paths\n",
    "from src.io_utils import make_run_dir, save_run_metadata, save_selected_run, pack_config, save_torch, save_csv, save_json, save_npz\n",
    "from src.metrics import residual_quality\n",
    "\n",
    "# ---------- config ----------\n",
    "ARTIFACTS_ROOT = str(PROJECT_ROOT / \"artifacts\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "params = ModelParams(device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "cfg_seed = 0\n",
    "cfg_probe = TrainConfig.full(seed=cfg_seed)\n",
    "run_dir = make_run_dir(ARTIFACTS_ROOT, \"commitment\", tag=cfg_probe.mode, seed=cfg_probe.seed)\n",
    "cfg_base = TrainConfig.full(seed=cfg_seed, run_dir=run_dir, artifacts_root=ARTIFACTS_ROOT)\n",
    "# Commitment-only training override for stability: easier warm-up in phase1, strict refinement in phase2.\n",
    "cfg = replace(\n",
    "    cfg_base,\n",
    "    strict_eps_max_steps=120_000,\n",
    "    n_path=192,\n",
    "    phase1=replace(cfg_base.phase1, eps_stop=1e-5),\n",
    "    phase2=replace(cfg_base.phase2, eps_stop=1e-8, batch_size=96, lr=1e-6),\n",
    ")\n",
    "\n",
    "save_run_metadata(run_dir, pack_config(params, cfg, extra={\"policy\":\"commitment\"}))\n",
    "print(\"Run dir:\", run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- model ----------\n",
    "d_in, d_out = 7, 13\n",
    "net = PolicyNetwork(d_in, d_out, hidden=cfg.hidden_layers, activation=cfg.activation)\n",
    "\n",
    "trainer = Trainer(\n",
    "    params=params,\n",
    "    cfg=cfg,\n",
    "    policy=\"commitment\",\n",
    "    net=net,\n",
    "    rbar_by_regime=None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d687a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- train ----------\n",
    "losses = trainer.train(\n",
    "    commitment_sss=None,\n",
    "    n_path=cfg.n_path,\n",
    "    n_paths_per_step=cfg.n_paths_per_step,\n",
    ")\n",
    "\n",
    "# save weights and log\n",
    "save_torch(os.path.join(run_dir, \"weights.pt\"), trainer.net.state_dict())\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"iter\": np.arange(len(losses)), \"loss\": losses})\n",
    "save_csv(os.path.join(run_dir, \"train_log.csv\"), df)\n",
    "\n",
    "# quality on a fresh validation batch sampled from the model's simulated state distribution\n",
    "# Discretion residuals require autograd through Delta-derivative terms.\n",
    "ctx = torch.enable_grad() if trainer.policy == \"discretion\" else torch.inference_mode()\n",
    "with ctx:\n",
    "    x_val = trainer.simulate_initial_state(int(cfg.val_size), commitment_sss=None)\n",
    "    # optional short burn-in for validation states (kept small; training itself is path-based)\n",
    "    val_burn = int(getattr(cfg, \"val_burn_in\", 200))\n",
    "    for _ in range(val_burn):\n",
    "        x_val = trainer._step_state(x_val)\n",
    "    resid = trainer._residuals(x_val).detach().cpu().numpy()\n",
    "q = residual_quality(resid, tol=getattr(cfg, \"report_tol\", 1e-3))\n",
    "save_json(os.path.join(run_dir, \"train_quality.json\"), q)\n",
    "print(\"Train quality:\", q)\n",
    "\n",
    "# Commitment SSS and timeless simulations are computed in the next cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18741503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Commitment SSS and diagnostics (timeless bootstrap + refinement) ----------\n",
    "from dataclasses import replace\n",
    "from src.steady_states import solve_commitment_sss_from_policy\n",
    "from src.sanity_checks import fixed_point_check, residuals_check_switching_consistent\n",
    "\n",
    "# 1) Bootstrap: compute switching-consistent SSS from the first-pass network.\n",
    "# This gives lagged multipliers (vartheta_prev, varrho_prev) needed for timeless initialization.\n",
    "comm_sss_bootstrap = solve_commitment_sss_from_policy(params, trainer.net)\n",
    "\n",
    "# 2) Timeless refinement: re-train with commitment states initialized at SSS multipliers.\n",
    "# We keep phase-2-only refinement (same equations/objective, cheaper than full re-train).\n",
    "timeless_refine_rounds = 1\n",
    "timeless_phase2_steps = int(cfg.phase2.steps)\n",
    "cfg_tl = replace(\n",
    "    cfg,\n",
    "    phase1=replace(cfg.phase1, steps=0),\n",
    "    phase2=replace(cfg.phase2, steps=timeless_phase2_steps),\n",
    ")\n",
    "\n",
    "timeless_losses = []\n",
    "comm_sss_final = comm_sss_bootstrap\n",
    "for rr in range(timeless_refine_rounds):\n",
    "    trainer = Trainer(\n",
    "        params=params,\n",
    "        cfg=cfg_tl,\n",
    "        policy=\"commitment\",\n",
    "        net=trainer.net,\n",
    "        rbar_by_regime=None,\n",
    "    )\n",
    "    ft_losses = trainer.train(\n",
    "        commitment_sss=comm_sss_final.by_regime,\n",
    "        n_path=cfg_tl.n_path,\n",
    "        n_paths_per_step=cfg_tl.n_paths_per_step,\n",
    "    )\n",
    "    timeless_losses.extend(ft_losses)\n",
    "    comm_sss_final = solve_commitment_sss_from_policy(params, trainer.net)\n",
    "    if len(ft_losses):\n",
    "        print(f\"Timeless refine round {rr+1}: steps={len(ft_losses)}, best_loss={min(ft_losses):.3e}\")\n",
    "    else:\n",
    "        print(f\"Timeless refine round {rr+1}: no extra steps were run.\")\n",
    "\n",
    "# Persist refined network and timeless-refine log\n",
    "save_torch(os.path.join(run_dir, \"weights.pt\"), trainer.net.state_dict())\n",
    "import pandas as pd\n",
    "save_csv(\n",
    "    os.path.join(run_dir, \"train_log_timeless_refine.csv\"),\n",
    "    pd.DataFrame({\"iter\": np.arange(len(timeless_losses)), \"loss\": timeless_losses}),\n",
    ")\n",
    "\n",
    "# Save final switching-consistent timeless SSS\n",
    "save_json(os.path.join(run_dir, 'sss_policy_fixed_point.json'), {'policy':'commitment','by_regime': comm_sss_final.by_regime})\n",
    "\n",
    "# Refresh validation quality on SSS-initialized commitment states\n",
    "with torch.inference_mode():\n",
    "    x_val = trainer.simulate_initial_state(int(cfg.val_size), commitment_sss=comm_sss_final.by_regime)\n",
    "    val_burn = int(getattr(cfg, \"val_burn_in\", 200))\n",
    "    for _ in range(val_burn):\n",
    "        x_val = trainer._step_state(x_val)\n",
    "    resid = trainer._residuals(x_val).detach().cpu().numpy()\n",
    "q = residual_quality(resid, tol=getattr(cfg, \"report_tol\", 1e-3))\n",
    "save_json(os.path.join(run_dir, \"train_quality.json\"), q)\n",
    "print(\"Train quality (after timeless refinement):\", q)\n",
    "\n",
    "print('=== COMMITMENT SSS (switching-consistent, includes lagged multipliers; timeless perspective) ===')\n",
    "for _s in sorted(comm_sss_final.by_regime.keys()):\n",
    "    print(f'Regime {_s}:')\n",
    "    for _k,_v in comm_sss_final.by_regime[_s].items():\n",
    "        print(f'{_k:>20}: {_v}')\n",
    "\n",
    "fp = fixed_point_check(params, trainer.net, policy='commitment', sss_by_regime=comm_sss_final.by_regime)\n",
    "rc = residuals_check_switching_consistent(params, trainer.net, policy='commitment', sss_by_regime=comm_sss_final.by_regime)\n",
    "print('Fixed-regime one-step check max |x_next-x| by regime (NOT Table-2 SSS):', {k:v.max_abs_state_diff for k,v in fp.items()})\n",
    "print('Switching-consistent residual check max |res| by regime:', {k:v.max_abs_residual for k,v in rc.items()})\n",
    "print('Residual keys:', list(next(iter(rc.values())).residuals.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save sanity checks ----------\n",
    "save_json(os.path.join(run_dir, 'sanity_checks.json'), {\n",
    "    'policy': 'commitment',\n",
    "    'fixed_regime_one_step_max_abs_state_diff': {int(k): float(v.max_abs_state_diff) for k,v in fp.items()},\n",
    "    'residual_max_abs': {int(k): float(v.max_abs_residual) for k,v in rc.items()},\n",
    "    'residuals_by_regime': {int(k): {kk: float(vv) for kk,vv in v.residuals.items()} for k,v in rc.items()},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Simulate (timeless commitment: start from SSS incl. lagged multipliers) ----------\n",
    "# This produces sim_paths.npz used by Table 2 / figures.\n",
    "B_sim = 512\n",
    "T_sim = 20000\n",
    "burn_in_sim = 2000\n",
    "\n",
    "x0_sim = trainer.simulate_initial_state(B_sim, commitment_sss=comm_sss_final.by_regime)\n",
    "sim = simulate_paths(\n",
    "    params=params,\n",
    "    policy=\"commitment\",\n",
    "    net=trainer.net,\n",
    "    T=T_sim,\n",
    "    burn_in=burn_in_sim,\n",
    "    x0=x0_sim,\n",
    "    compute_implied_i=True,\n",
    "    gh_n=3,\n",
    "    thin=1,\n",
    "    show_progress=True,\n",
    "    store_states=False,\n",
    ")\n",
    "\n",
    "save_npz(os.path.join(run_dir, \"sim_paths.npz\"), **sim)\n",
    "print(\"Saved sim_paths:\", os.path.join(run_dir, \"sim_paths.npz\"))\n",
    "\n",
    "# Mark run as selected only after all core artifacts are saved.\n",
    "save_selected_run(ARTIFACTS_ROOT, trainer.policy, run_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
